{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from datamodules.transformations import UnNest\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from torchvision.transforms import Compose\n",
    "from tqdm.auto import tqdm\n",
    "from datamodules.image_classification import CIFAR10DataModule\n",
    "from models.interpretation import ImageInterpretationNet\n",
    "from utils.getters_setters import vit_getter, vit_setter\n",
    "from attributions.grad_cam import grad_cam\n",
    "from attributions.attention_rollout import attention_rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CIFAR-10 Test Split and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "vit = ViTForImageClassification.from_pretrained(\"tanlq/vit-base-patch16-224-in21k-finetuned-cifar10\").to(device)\n",
    "\n",
    "diffmask = ImageInterpretationNet.load_from_checkpoint('diffmask.ckpt').to(device)\n",
    "\n",
    "feature_extractor=ViTFeatureExtractor.from_pretrained(\n",
    "    \"tanlq/vit-base-patch16-224-in21k-finetuned-cifar10\", return_tensors=\"pt\"\n",
    ")\n",
    "feature_extractor = UnNest(feature_extractor)\n",
    "\n",
    "dm = CIFAR10DataModule(feature_extractor=feature_extractor, batch_size=16)\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "dataloader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to measure KL-divergence between masked & unmasked input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def kl_divergence(model, input, mask, b, n_hidden = 14, patch_size=16):\n",
    "    # Reshape mask to hidden states' shape\n",
    "    B, H, W = mask.shape\n",
    "    mask = mask.reshape(B, 1, H, W)\n",
    "    mask = F.interpolate(mask, scale_factor=1/patch_size)\n",
    "    mask = mask.reshape(B, -1, 1)\n",
    "    \n",
    "    # Get hidden states from unmasked input\n",
    "    logits_orig, hidden_states = vit_getter(model, input)\n",
    "    \n",
    "    # Calculate hidden states from masked input\n",
    "    new_hidden_states = hidden_states[0] * mask + b * (1 - mask)        \n",
    "    new_hidden_states = [new_hidden_states] + [None] * (n_hidden - 1)\n",
    "    \n",
    "    # Append CLS token\n",
    "    cls_tokens = model.vit.embeddings.cls_token.expand(B, -1, -1)\n",
    "    new_hidden_states[0] = torch.cat((cls_tokens, new_hidden_states[0]), dim=1)\n",
    "\n",
    "    # Get logits from new hidden states (masked input)\n",
    "    logits, _ = vit_setter(model, input, new_hidden_states)\n",
    "\n",
    "    # Compute KL divergence between the logits from the original and the masked input\n",
    "    kl_div = torch.distributions.kl_divergence(\n",
    "                torch.distributions.Categorical(logits=logits_orig),\n",
    "                torch.distributions.Categorical(logits=logits),\n",
    "            )\n",
    "    \n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-Divergence for Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klds = []\n",
    "masks_percentage = []\n",
    "\n",
    "for images, _ in tqdm(dataloader):\n",
    "    images = images.cuda()\n",
    "    gradcam_masks = grad_cam(images, vit, True if device=='cuda' else False)\n",
    "    masked_pixels_percentages = [100 * (1 - mask.mean(-1).mean(-1).item()) for mask in gradcam_masks]\n",
    "    klds.append(kl_divergence(vit, images, gradcam_masks.cuda(), diffmask.gate.placeholder))\n",
    "    masks_percentage.extend(masked_pixels_percentages)\n",
    "\n",
    "    \n",
    "klds = torch.cat(klds)\n",
    "print(f\"Grad-CAM mean KL-Divergence: {klds.mean()}\")\n",
    "print(f\"Masking percentage: {np.mean(masks_percentage)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-Divergence for Attention Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klds = []\n",
    "masks_percentage = []\n",
    "\n",
    "for images, _ in tqdm(dataloader):\n",
    "    images = images.cuda()\n",
    "    rollout_masks = attention_rollout(images=images, vit=vit, device=device)\n",
    "    masked_pixels_percentages = [100 * (1 - mask.mean(-1).mean(-1).item()) for mask in rollout_masks]\n",
    "    klds.append(kl_divergence(vit, images, rollout_masks, diffmask.gate.placeholder))\n",
    "    masks_percentage.extend(masked_pixels_percentages)\n",
    "\n",
    "    \n",
    "klds = torch.cat(klds)\n",
    "print(f\"Rollout mean KL-Divergence: {klds.mean()}\")\n",
    "print(f\"Masking percentage: {np.mean(masks_percentage)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-Divergence for DiffMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffmask.set_vision_transformer(vit)\n",
    "\n",
    "klds = []\n",
    "masks_percentage = []\n",
    "\n",
    "for images, _ in tqdm(dataloader):\n",
    "    images = images.cuda()\n",
    "    diff_masks = diffmask.get_mask(images)[\"mask\"].detach()\n",
    "    masked_pixels_percentages = [100 * (1 - mask.mean(-1).mean(-1).item()) for mask in diff_masks]\n",
    "    klds.append(kl_divergence(vit, images, diff_masks, diffmask.gate.placeholder))\n",
    "    masks_percentage.extend(masked_pixels_percentages)\n",
    "    \n",
    "klds = torch.cat(klds)\n",
    "print(f\"Diffmask mean KL-Divergence: {klds.mean()}\")\n",
    "print(f\"Masking percentage: {np.mean(masks_percentage)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b473823a025b6c28dc6523e3acf459442e5c888fcddecc1952c1c9fd9cecce0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
