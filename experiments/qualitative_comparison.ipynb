{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms.functional import normalize\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from models.interpretation import ImageInterpretationNet\n",
    "from datamodules.image_classification import CIFAR10DataModule\n",
    "from datamodules.transformations import UnNest\n",
    "from attributions.grad_cam import grad_cam\n",
    "from attributions.attention_rollout import attention_rollout\n",
    "from matplotlib import pyplot as plt\n",
    "from utils.plot import smoothen, draw_mask_on_image, draw_heatmap_on_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sample Images and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load pre-trained Transformer\n",
    "vit = ViTForImageClassification.from_pretrained(\"tanlq/vit-base-patch16-224-in21k-finetuned-cifar10\")\n",
    "vit.eval()\n",
    "\n",
    "# Load images\n",
    "feature_extractor=ViTFeatureExtractor.from_pretrained(\n",
    "    \"tanlq/vit-base-patch16-224-in21k-finetuned-cifar10\", return_tensors=\"pt\"\n",
    ")\n",
    "feature_extractor = UnNest(feature_extractor)\n",
    "\n",
    "dm = CIFAR10DataModule(feature_extractor=feature_extractor, batch_size=800)\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "dt = iter(dm.test_dataloader())\n",
    "images, labels = next(dt)\n",
    "\n",
    "indices = [235, 330, 67, 760, 227, 141, 29, 56, 80, 170]\n",
    "\n",
    "images, labels = images[indices], labels[indices]\n",
    "rgb_images = [normalize(image, [-0.5, -0.5, -0.5], [2, 2, 2]).permute(1, 2, 0).clip(0, 1) for image in images] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute GradCAM masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcam_masks = grad_cam(images, vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Attention Rollout masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attentions from images\n",
    "attentions = vit(images, output_attentions=True).attentions\n",
    "\n",
    "# Compute attention rollout masks\n",
    "rollout_masks = attention_rollout(images=images, vit=vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute DiffMask masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interpretation model\n",
    "diffmask = ImageInterpretationNet.load_from_checkpoint('diffmask.ckpt')\n",
    "diffmask.set_vision_transformer(vit)\n",
    "\n",
    "# Compute diffmasks\n",
    "diff_masks = diffmask.get_mask(images)[\"mask\"].detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "draw_mask = lambda image, mask: draw_mask_on_image(image.permute(2, 0, 1), smoothen(mask)).permute(1, 2, 0).clip(0, 1).numpy()\n",
    "draw_heatmap = lambda image, mask: draw_heatmap_on_image(image.permute(2, 0, 1), smoothen(mask)).permute(1, 2, 0).clip(0, 1).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(7, 10, figsize=(20, 14))\n",
    "\n",
    "# Remove axes\n",
    "for i in range(7):\n",
    "    for j in range(10):\n",
    "            ax[i, j].set_xticks([])\n",
    "            ax[i, j].set_yticks([])\n",
    "\n",
    "font_size = 20\n",
    "for i in range(10):\n",
    "    # Original image\n",
    "    if i == 0:\n",
    "        ax[0, i].set_ylabel(\"Input\", fontsize=font_size)\n",
    "        ax[1, i].set_ylabel(\"Attn. Roll.\", fontsize=font_size)\n",
    "        ax[2, i].set_ylabel(\"Grad-CAM\", fontsize=font_size)\n",
    "        ax[3, i].set_ylabel(\"Ours\", fontsize=font_size)\n",
    "        ax[4, i].set_ylabel(\"Attn. Roll.\", fontsize=font_size)\n",
    "        ax[5, i].set_ylabel(\"Grad-CAM\", fontsize=font_size)\n",
    "        ax[6, i].set_ylabel(\"Ours\", fontsize=font_size)\n",
    "        \n",
    "    ax[0, i].imshow(rgb_images[i])\n",
    "    \n",
    "    ax[1, i].imshow(draw_mask(rgb_images[i], rollout_masks[i]))\n",
    "    ax[2, i].imshow(draw_mask(rgb_images[i], gradcam_masks[i]))\n",
    "    ax[3, i].imshow(draw_mask(rgb_images[i], diff_masks[i].detach()))\n",
    "    \n",
    "    ax[4, i].imshow(draw_heatmap(rgb_images[i], rollout_masks[i]))\n",
    "    ax[5, i].imshow(draw_heatmap(rgb_images[i], gradcam_masks[i]))\n",
    "    ax[6, i].imshow(draw_heatmap(rgb_images[i], diff_masks[i].detach()))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"qualitative_comparison.jpg\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b473823a025b6c28dc6523e3acf459442e5c888fcddecc1952c1c9fd9cecce0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
