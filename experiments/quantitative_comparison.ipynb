{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from datamodules.transformations import UnNest\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from tqdm.auto import tqdm\n",
    "from datamodules.image_classification import CIFAR10DataModule\n",
    "from models.interpretation import ImageInterpretationNet\n",
    "from utils.getters_setters import vit_getter, vit_setter\n",
    "from attributions.grad_cam import grad_cam\n",
    "from attributions.attention_rollout import attention_rollout\n",
    "from utils.plot import smoothen\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load CIFAR-10 Test Split and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "vit = ViTForImageClassification.from_pretrained(\"tanlq/vit-base-patch16-224-in21k-finetuned-cifar10\").to(device)\n",
    "\n",
    "diffmask = ImageInterpretationNet.load_from_checkpoint('../checkpoints/diffmask.ckpt').to(device)\n",
    "\n",
    "feature_extractor=ViTFeatureExtractor.from_pretrained(\n",
    "    \"tanlq/vit-base-patch16-224-in21k-finetuned-cifar10\", return_tensors=\"pt\"\n",
    ")\n",
    "feature_extractor = UnNest(feature_extractor)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "dm = CIFAR10DataModule(feature_extractor=feature_extractor, batch_size=batch_size)\n",
    "dm.prepare_data()\n",
    "dm.setup('test')\n",
    "dataloader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to measure area under curve (AUC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_index(index, shape):\n",
    "        out = []\n",
    "        for dim in reversed(shape):\n",
    "            out.append(index % dim)\n",
    "            index = torch.div(index, dim, rounding_mode='trunc')\n",
    "        return tuple(reversed(out))\n",
    "\n",
    "def auc(model, input, labels, attributions, positive=True, k=0.2, num_tokens=1):\n",
    "    area = []\n",
    "    B, H, W = attributions.shape\n",
    "    attributions = attributions.reshape(B, -1)\n",
    "    # Get hidden states from unmasked input\n",
    "    logits_orig, hidden_states = vit_getter(model, input)\n",
    "    probs_orig = logits_orig.softmax(-1)\n",
    "    if not labels:\n",
    "        labels = probs_orig.argmax(-1)\n",
    "\n",
    "    drop_k = int(k * attributions.shape[1])\n",
    "\n",
    "    for i in range(0, drop_k + 1, 256 * num_tokens):\n",
    "        keep_k = attributions.shape[1] - i\n",
    "        idx = attributions.topk(keep_k, dim=1, largest=(not positive)).indices.sort()[0].squeeze(-1)\n",
    "\n",
    "        torch.manual_seed(123)\n",
    "        f = torch.rand((B, input.shape[1], H, W), device=device)*2 - 1 # Baseline pixels\n",
    "        for i in range(B):\n",
    "            idx_i = idx[i]\n",
    "            unraveled_idx = unravel_index(idx_i, (H, W))\n",
    "            f[i, :, unraveled_idx[0], unraveled_idx[1]] = input[i, :, unraveled_idx[0], unraveled_idx[1]]\n",
    "\n",
    "        # Forward pass through the model to get the logits\n",
    "        logits, hidden_states = vit_getter(model, f)\n",
    "        probs = logits.softmax(-1)\n",
    "\n",
    "        area.append(probs[range(B), labels].mean().item())\n",
    "\n",
    "    return area\n",
    "\n",
    "def calculate_auc(positive=True, method='diffmask'):\n",
    "    auc_positive = []\n",
    "    for i, (images, labels) in tqdm(enumerate(dataloader), total=50):\n",
    "        if i == 50: break\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        if method == 'diffmask':\n",
    "            attributions = diffmask.get_mask(images)[\"mask\"].detach()\n",
    "        elif method == 'rollout':\n",
    "            attributions = attention_rollout(images=images, vit=vit, device=device)\n",
    "        elif method == 'gradcam':\n",
    "            images.requires_grad = True\n",
    "            attributions = grad_cam(images, vit, True if device=='cuda' else False)\n",
    "        comp = auc(vit, images, None, attributions, num_tokens=7, k=1, positive=positive)\n",
    "        auc_positive.append(comp)\n",
    "    return auc_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC for Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_pos_gradcam = np.asarray(calculate_auc(positive=True, method='gradcam')).mean(0)\n",
    "auc_neg_gradcam = np.asarray(calculate_auc(positive=False, method='gradcam')).mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC for DiffMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffmask.set_vision_transformer(vit)\n",
    "\n",
    "auc_pos_diffmask = np.asarray(calculate_auc(positive=True)).mean(0)\n",
    "auc_neg_diffmask = np.asarray(calculate_auc(positive=False)).mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC for Attention-Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_pos_rollout = np.asarray(calculate_auc(positive=True, method='rollout')).mean(0)\n",
    "auc_neg_rollout = np.asarray(calculate_auc(positive=False, method='rollout')).mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(auc_pos_diffmask, label=\"DiffMask\")\n",
    "plt.plot(auc_pos_rollout, label=\"Attention Rollout\")\n",
    "plt.plot(auc_pos_gradcam, label=\"Grad-CAM\")\n",
    "plt.xlabel(\"Percentage of positive pixels removed\")\n",
    "len_x = len(auc_pos_diffmask)\n",
    "x_ticks = np.arange(0, len_x, 3)\n",
    "x_ticks_labels = [str(int(x * 100 / len_x)) + \"%\" for x in x_ticks]\n",
    "plt.xticks(x_ticks, x_ticks_labels)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.plot(auc_neg_diffmask, label=\"DiffMask\")\n",
    "plt.plot(auc_neg_rollout, label=\"Attention Rollout\")\n",
    "plt.plot(auc_neg_gradcam, label=\"Grad-CAM\")\n",
    "len_x = len(auc_neg_diffmask)\n",
    "x_ticks = np.arange(0, len_x, 3)\n",
    "x_ticks_labels = [str(int(x * 100 / len_x)) + \"%\" for x in x_ticks]\n",
    "plt.xticks(x_ticks, x_ticks_labels)\n",
    "plt.xlabel(\"Percentage of negative pixels removed\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and KL-Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Function to measure KL-divergence and Accuracy between masked & unmasked input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_kld_and_acc(model, input, labels, mask, b, n_hidden=14, patch_size=16):\n",
    "    # Reshape mask to hidden states' shape\n",
    "    B, H, W = mask.shape\n",
    "    mask = mask.reshape(B, 1, H, W)\n",
    "    mask = F.interpolate(mask, scale_factor=1/patch_size)\n",
    "    mask = mask.reshape(B, -1, 1)\n",
    "    \n",
    "    # Get hidden states from unmasked input\n",
    "    logits_orig, hidden_states = vit_getter(model, input)\n",
    "    \n",
    "    # Calculate hidden states from masked input\n",
    "    new_hidden_states = hidden_states[0] * mask + b * (1 - mask)\n",
    "    new_hidden_states = [new_hidden_states] + [None] * (n_hidden - 1)\n",
    "    \n",
    "    # Append CLS token\n",
    "    cls_tokens = model.vit.embeddings.cls_token.expand(B, -1, -1)\n",
    "    new_hidden_states[0] = torch.cat((cls_tokens, new_hidden_states[0]), dim=1)\n",
    "\n",
    "    # Get logits from new hidden states (masked input)\n",
    "    logits, _ = vit_setter(model, input, new_hidden_states)\n",
    "    \n",
    "    pred_class = logits.argmax(-1)\n",
    "    acc = pred_class == labels\n",
    "\n",
    "    # Compute KL divergence between the logits from the original and the masked input\n",
    "    kl_div = torch.distributions.kl_divergence(\n",
    "                torch.distributions.Categorical(logits=logits_orig),\n",
    "                torch.distributions.Categorical(logits=logits),\n",
    "            )\n",
    "    \n",
    "    return kl_div, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### KL-Divergence and Accuracy for Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "klds = []\n",
    "accs = []\n",
    "masks_percentage = []\n",
    "\n",
    "for images, labels in tqdm(dataloader):\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "    gradcam_masks = grad_cam(images, vit, True if device=='cuda' else False)\n",
    "    masked_pixels_percentages = [100 * (1 - mask.mean(-1).mean(-1).item()) for mask in gradcam_masks]\n",
    "    kld, acc = get_kld_and_acc(vit, images, labels, gradcam_masks.cuda(), diffmask.gate.placeholder)\n",
    "    klds.append(kld)\n",
    "    accs.append(acc)\n",
    "    masks_percentage.extend(masked_pixels_percentages)\n",
    "\n",
    "    \n",
    "klds = torch.cat(klds)\n",
    "accs = torch.cat(accs)\n",
    "print(f\"Grad-CAM mean KL-Divergence: {klds.mean()}\")\n",
    "print(f\"Grad-CAM accuracy: {torch.sum(accs)/ len(accs)}\")\n",
    "print(f\"Masking percentage: {np.mean(masks_percentage)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### KL-Divergence and Accuracy for Attention Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "klds = []\n",
    "accs = []\n",
    "masks_percentage = []\n",
    "\n",
    "for images, labels in tqdm(dataloader):\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "    rollout_masks = attention_rollout(images=images, vit=vit, device=device)\n",
    "    masked_pixels_percentages = [100 * (1 - mask.mean(-1).mean(-1).item()) for mask in rollout_masks]\n",
    "    kld, acc = get_kld_and_acc(vit, images, labels, rollout_masks, diffmask.gate.placeholder)\n",
    "    klds.append(kld)\n",
    "    accs.append(acc)\n",
    "    masks_percentage.extend(masked_pixels_percentages)\n",
    "\n",
    "    \n",
    "klds = torch.cat(klds)\n",
    "accs = torch.cat(accs)\n",
    "print(f\"Rollout mean KL-Divergence: {klds.mean()}\")\n",
    "print(f\"Rollout accuracy: {torch.sum(accs)/ len(accs)}\")\n",
    "print(f\"Masking percentage: {np.mean(masks_percentage)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### KL-Divergence and Accuracy for DiffMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffmask.set_vision_transformer(vit)\n",
    "\n",
    "klds = []\n",
    "accs = []\n",
    "masks_percentage = []\n",
    "\n",
    "for i, (images, labels) in enumerate(tqdm(dataloader)):\n",
    "    if i == 10: break\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "    diff_masks = diffmask.get_mask(images)[\"mask\"].detach()\n",
    "    masked_pixels_percentages = [100 * (1 - mask.mean(-1).mean(-1).item()) for mask in diff_masks]\n",
    "    kld, acc = get_kld_and_acc(vit, images, labels, diff_masks, diffmask.gate.placeholder)\n",
    "    klds.append(kld)\n",
    "    accs.append(acc)\n",
    "    masks_percentage.extend(masked_pixels_percentages)\n",
    "\n",
    "klds = torch.cat(klds)\n",
    "accs = torch.cat(accs)\n",
    "print(f\"Diffmask mean KL-Divergence: {klds.mean()}\")\n",
    "print(f\"Diffmask accuracy: {torch.sum(accs)/ len(accs)}\")\n",
    "print(f\"Masking percentage: {np.mean(masks_percentage)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f313bec56c1976e4c29c99905da437d30c1f75edce67ef26f4d41ac4d75ee169"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
